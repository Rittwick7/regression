{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        " - Simple linear regression is a statistical method for establishing the relationship between two variables using a straight line. The line is drawn by finding the slope and intercept, which define the line and minimize regression errors.\n",
        "  The simplest form of simple linear regression has only one x variable and one y variable. The x variable is the independent variable because it is independent of what you try to predict the dependent variable. The y variable is the dependent variable because it depends on what you try to predict.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        " - Simple linear regression relies on key assumptions, often remembered as LINE: Linear relationship between variables, Independent observations, Normally distributed residuals, and Equal variance of residuals (homoscedasticity). These assumptions ensure the model's coefficient estimates, confidence intervals, and p-values are reliable and accurate for prediction.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        " -\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        " -\n",
        "\n",
        "5.  How do we calculate the slope m in Simple Linear Regression?\n",
        " -\n",
        "\n",
        "6.  What is the purpose of the least squares method in Simple Linear Regression?\n",
        " - The purpose of the least squares method in Simple Linear Regression is to determine the \"line of best fit\" for a dataset by minimizing the sum of the squared vertical distances (residuals) between the observed data points and the regression line. It calculates the optimal slope (m) and intercept (b) to create a predictive model that minimizes errors.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        " - R² measures the goodness of fit of the regression line. It ranges from 0 to 1 (or 0% to 100%)\n",
        " R²=0 : The model explains none of the variability of the response data around its mean.\n",
        "\n",
        " R²=1 : The model explains all the variability of the response data around its mean.\n",
        "Example: An R² of 0.75 indicates that 75% of the variation in the dependent variable is accounted for by the independent variable, while the remaining 25% is attributed to inherent randomness or variables not included in the model.\n",
        "8. What is Multiple Linear Regression?\n",
        " - Multiple linear regression (MLR) is a statistical technique that models the relationship between one continuous dependent variable and two or more independent (predictor) variables by fitting a linear equation to observed data. It predicts outcomes and identifies the strength of the relationship between variables.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        " - Number of Variables: Simple linear regression uses one independent variable, while multiple linear regression uses two or more. Complexity: Multiple linear regression is more complex as it accounts for multiple factors and their combined effect on the dependent variable.\n",
        "\n",
        "10.  What are the key assumptions of Multiple Linear Regression?\n",
        " - Multiple linear regression requires a linear relationship between predictors and the outcome, no high multicollinearity among predictors, and independent observations. Key assumptions also include homoscedasticity (constant variance of residuals), normal distribution of error terms, and that the model is properly specified.\n",
        "Key Assumptions Detailed:\n",
        "Linearity: The relationship between the independent variables (X) and the dependent variable (Y) is linear.\n",
        "No/Low Multicollinearity: Independent variables should not be highly correlated with each other, which can be checked using Variance Inflation Factor (VIF).\n",
        "Homoscedasticity: The residuals (errors) maintain constant variance at every level of the independent variables.\n",
        "Independence of Errors: Observations are independent, with no autocorrelation between consecutive residuals (especially in time-series data).\n",
        "Normality of Errors: The residuals of the model are normally distributed.\n",
        "No Endogeneity: The independent variables are not correlated with the error term.\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        " - Heteroscedasticity occurs in multiple linear regression when the variance of the error terms (residuals) is not constant across all levels of the independent variables, causing a \"fan\" or \"cone\" shape in residual plots. While it does not bias coefficient estimates, it violates OLS assumptions, making estimates inefficient, invalidating standard errors, and rendering hypothesis tests (t-tests, F-tests) unreliable.\n",
        "Impact on Multiple Linear Regression Results\n",
        "Inefficient Estimates: Although coefficients remain unbiased (centered around the true value), they are no longer the \"Best\" Linear Unbiased Estimators (BLUE). Other methods could provide more precise estimates.\n",
        "Invalid Hypothesis Testing: Because standard errors are calculated incorrectly,\n",
        "-values and confidence intervals become unreliable. You might incorrectly conclude that a variable is statistically significant (or not).\n",
        "Misleading Confidence Intervals: Confidence intervals may be too wide or too narrow, reducing the trustworthiness of predictions.\n",
        "Increased Variance: The precision of the regression coefficient estimates decreases.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        " - Improving a Multiple Linear Regression model with high multicollinearity—where predictor variables are strongly correlated—requires reducing redundancy among independent variables to stabilize coefficient estimates. Multicollinearity makes models unstable and difficult to interpret, as it causes large standard errors and makes individual p-values unreliable.\n",
        "  Here are the primary methods to improve such a model, ranked from simplest to most advanced:\n",
        "  1. Remove Highly Correlated Predictors\n",
        "Use VIF (Variance Inflation Factor): Identify variables with VIF values higher than 5 or 10.\n",
        "Drop Redundant Features: Remove one of the variables in a pair that is highly correlated (e.g., if \"weight\" and \"body surface area\" are correlated, remove one).\n",
        "Action: Iteratively remove the variable with the highest VIF and recalculate VIF for the remaining variables until all are below a desired threshold.\n",
        "2. Feature Engineering and Combination\n",
        "Combine Variables: Instead of using two highly correlated variables, combine them into a single, more meaningful predictor (e.g., take the average or sum).\n",
        "Feature Transformation: Use domain knowledge to create a ratio or a new metric that represents the underlying construct better than the raw, correlated variables.\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        " - Transforming categorical variables into numerical formats is a crucial step for regression models, which require numerical input to calculate coefficients. Common techniques include One-Hot/Dummy Encoding for nominal data, Ordinal Encoding for ranked data, and Target Encoding for high-cardinality features.\n",
        "Here are the most common techniques for transforming categorical variables for regression models:\n",
        "1. One-Hot Encoding (OHE) and Dummy Coding\n",
        "These methods convert categorical variables into binary vectors (0 or 1), representing the presence or absence of a category.\n",
        "One-Hot Encoding: Creates N binary columns for a feature with N unique categories. It is ideal for nominal data (no natural order) but can cause high dimensionality.\n",
        "Dummy Coding: An improvement for linear regression that creates N-1 features, dropping one category to serve as the reference (baseline) level. This avoids the \"dummy variable trap\" or multicollinearity.\n",
        "2. Ordinal Encoding\n",
        "Used when categories have a natural, inherent ranking (e.g., \"low,\" \"medium,\" \"high\").\n",
        "Technique: Maps each category to a specific integer based on its rank (e.g., Low=1, Medium=2, High=3).\n",
        "Use Case: Preserves the order of the data. If applied to unordered data, it may incorrectly imply a relationship.\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        " - Interaction terms in multiple linear regression (MLR) allow the model to capture situations where the effect of one predictor variable on the response variable depends on the level of another predictor. They signify that the relationship is not merely additive, improving model flexibility and accuracy by allowing slopes to vary.\n",
        "\n",
        "15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        " -\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        " - Direction of Relationship: A positive slope indicates y increases as x increases, while a negative slope indicates x decreases as y increases.\n",
        "Rate of Change: The magnitude of the slope represents the specific unit change in x for each unit change in x\n",
        ". For example, a slope of\n",
        " means a\n",
        "-unit increase in\n",
        " causes a\n",
        "-unit increase in\n",
        ".\n",
        "Strength of Association: A larger absolute value of the slope typically indicates a stronger relationship.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        " -\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        " - A large standard error (SE) for a regression coefficient indicates that the estimated coefficient is imprecise and unreliable, suggesting high variability in the sample data. It means the predictor variable has low statistical power to explain the dependent variable, often leading to a non-significant, unstable, or statistically insignificant result.\n",
        "\n",
        "20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        " - Heteroscedasticity is identified in residual plots by a distinct non-random, fan, or cone-shaped pattern, where the spread of residuals increases or decreases with predicted values (fitted values). It is crucial to address because it violates ordinary least squares (OLS) assumptions, making standard errors invalid, hypothesis tests unreliable, and the model inefficient.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        " - Scaling variables in Multiple Linear Regression is important to ensure that features with larger magnitudes do not dominate the model, allowing for accurate coefficient interpretation, faster optimization convergence, and improved performance with regularization techniques. It standardizes independent variables to a common range, ensuring equal influence during model training.\n",
        "\n",
        "23. What is polynomial regression?\n",
        " - Polynomial regression is useful when the relationship between the variables exhibits curvature or when a linear model fails to capture the data adequately. It allows us to capture complex phenomena like diminishing returns, saturation effects, or exponential growth. Polynomial regression has applications in physics, economics, social sciences, and engineering.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        " -\n",
        "\n",
        "25. When is polynomial regression used?\n",
        " - Polynomial regression is used when the relationship between the independent and dependent variables is non-linear, meaning data points follow a curved trend rather than a straight line. It is applied to better fit data that shows acceleration, deceleration, or peaks, which simple linear regression cannot accurately capture.\n",
        "Key scenarios for using polynomial regression include:\n",
        "Non-Linear Relationships: When data trends resemble parabolas, curves, or S-shapes, such as growth rates, disease spread, or sales trends.\n",
        "Improved Model Performance: Used when residual plots from a linear model show patterns, suggesting a linear fit is inadequate and a curve is needed.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        " -\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        " -\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        " - Polynomial regression, while flexible for modeling non-linear data, is primarily limited by high risks of overfitting with higher degrees, leading to poor generalization on new data. It suffers from extreme instability and \"exploding\" predictions when extrapolating outside the training data range.\n",
        " Key Limitations:\n",
        "Overfitting: High-degree polynomials can fit noise instead of the underlying trend, creating a model that works perfectly on training data but poorly on test data.\n",
        "Extrapolation Issues: The model behaves unpredictably and often generates nonsensical, volatile predictions outside the observed data range.\n",
        "Sensitivity to Outliers: Extreme values can disproportionately influence the shape of the polynomial curve, reducing accuracy.\n",
        "Computational Complexity:As the degree and number of features increase, the number of terms grows significantly, causing higher computational costs.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        " -\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        " - Visualization is essential in polynomial regression to determine the optimal model complexity, as it helps identify non-linear patterns, detect overfitting or underfitting, and compare the fitted curve against raw data. It transforms abstract, multi-dimensional coefficients into interpretable graphs, making it easier to see how well the model captures turning points or local extrema.\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        " -\n"
      ],
      "metadata": {
        "id": "lKamo1JgqnZv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR72yq_3qkwP"
      },
      "outputs": [],
      "source": []
    }
  ]
}